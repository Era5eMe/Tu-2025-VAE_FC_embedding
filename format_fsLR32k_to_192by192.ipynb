{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wheelock/data1/people/Chenyan/Tu-2025-VAE_FC_embedding/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "from fMRIVAE_Model import BetaVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifti2_filename(cifti1_path, output_dir):\n",
    "    basename = os.path.basename(cifti1_path)\n",
    "\n",
    "    # List of known CIFTI suffixes\n",
    "    known_suffixes = [\".dtseries.nii\", \".dconn.nii\", \".dscalar.nii\", \".dpconn.nii\"]\n",
    "\n",
    "    # Match and strip full suffix\n",
    "    for suffix in known_suffixes:\n",
    "        if basename.endswith(suffix):\n",
    "            stem = basename[: -len(suffix)]  # strip the full suffix\n",
    "            cifti2_name = f\"{stem}_cifti2{suffix}\"\n",
    "            return os.path.join(output_dir, cifti2_name)\n",
    "\n",
    "    raise ValueError(\"Unknown CIFTI suffix in filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlations(input_file, tmask):\n",
    "    if input_file.endswith(\".dtseries.nii\"):\n",
    "        cifti_img = nib.load(input_file)\n",
    "        cifti_data = cifti_img.get_fdata()\n",
    "        # cifti_header = cifti_img.header\n",
    "        bm_index_map = cifti_img.header.get_index_map(1)\n",
    "\n",
    "        # Initialize storage\n",
    "        vertex_indices = []\n",
    "        # Loop through the brain models in the index map\n",
    "        for bm in bm_index_map.brain_models:\n",
    "            structure = bm.brain_structure\n",
    "            if structure in ['CIFTI_STRUCTURE_CORTEX_LEFT', 'CIFTI_STRUCTURE_CORTEX_RIGHT']:\n",
    "                # Convert Cifti2VertexIndices to numpy array\n",
    "                offset = bm.index_offset\n",
    "                count = bm.index_count\n",
    "                vertex_indices.extend(range(offset, offset + count))\n",
    "\n",
    "        vertex_indices = np.array(vertex_indices)\n",
    "        cortex_ts = np.transpose(cifti_data[:, vertex_indices])\n",
    "        masked_cortex_ts = cortex_ts[:, tmask]\n",
    "        corr_matrix = np.corrcoef(masked_cortex_ts)\n",
    "        return corr_matrix\n",
    "    else:\n",
    "        raise ValueError(\"Input file must be a CIFTI-2 time-series file. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_reformatting(corrs, transmat_path, img_size):\n",
    "    # assert corrs.shape == (59412, )\n",
    "    left_data, right_data = corrs[:29696, :], corrs[29696:, :]\n",
    "    left_mask = sio.loadmat(os.path.join(transmat_path, \"Left_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "    right_mask = sio.loadmat(os.path.join(transmat_path, \"Right_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "    left_transmat = left_mask['grid_mapping']\n",
    "    right_transmat = right_mask['grid_mapping']\n",
    "    \n",
    "    left_surf_data = np.reshape(left_transmat @ left_data, (img_size, img_size, 1, -1), order='F')\n",
    "    left_surf_data = np.transpose(left_surf_data, axes=(3, 2, 0, 1)) # (batch, 1, height_width)\n",
    "    right_surf_data = np.reshape(right_transmat @ right_data, (img_size, img_size, 1, -1), order='F')\n",
    "    right_surf_data = np.transpose(right_surf_data, axes=(3, 2, 0, 1))\n",
    "\n",
    "    return left_surf_data, right_surf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backword_reformatting(left_surf_recon, right_surf_recon, transmat_path):\n",
    "    assert left_surf_recon.shape == right_surf_recon.shape\n",
    "    batch_size = left_surf_recon.shape[0]\n",
    "    left_mask = sio.loadmat(os.path.join(transmat_path, \"Left_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "    right_mask = sio.loadmat(os.path.join(transmat_path, \"Right_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "    left_transmat_backward = left_mask['inverse_transformation']\n",
    "    right_transmat_backward = right_mask['inverse_transformation']\n",
    "\n",
    "    left_corrs = left_transmat_backward @ left_surf_recon.reshape(batch_size, -1).T\n",
    "    right_corrs = right_transmat_backward @ right_surf_recon.reshape(batch_size, -1).T\n",
    "    dtseries_recon = np.vstack((left_corrs, right_corrs))\n",
    "    dtseries_recon[dtseries_recon == 0] = 1\n",
    "    return dtseries_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_sample(left_surf_data, right_surf_data, sample_ratio=1.0):\n",
    "    assert left_surf_data.shape == right_surf_data.shape\n",
    "    if sample_ratio <= 0 or sample_ratio > 1.0:\n",
    "        raise ValueError(\"Please pick a sample ratio between 0 and 1. \")\n",
    "    # print(left_surf_data.shape)\n",
    "    sample_size, _, _, img_size = left_surf_data.shape\n",
    "    indices = np.random.choice(sample_size, int(sample_size*sample_ratio), replace=False)\n",
    "    sampled_left_surf_data = left_surf_data[indices, :, :, :]\n",
    "    sampled_right_surf_data  = right_surf_data[indices, :, :, :]\n",
    "    return sampled_left_surf_data, sampled_right_surf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(zdim, nc, device):\n",
    "    repo_id = \"cindyhfls/fcMRI-VAE\"\n",
    "    if zdim == 2:\n",
    "        filename = \"Checkpoint/checkpoint49_2024-03-28_Zdim_2_Vae-beta_20.0_Lr_0.0001_Batch-size_128_washu120_subsample10_train100_val10.pth.tar\"\n",
    "    elif zdim == 3:\n",
    "        filename = \"Checkpoint/checkpoint49_2024-11-28_Zdim_3_Vae-beta_20.0_Lr_0.0001_Batch-size_128_washu120_subsample10_train100_val10.pth.tar\"\n",
    "    elif zdim == 4:\n",
    "        filename = \"Checkpoint/checkpoint49_2024-06-21_Zdim_4_Vae-beta_20.0_Lr_0.0001_Batch-size_128_washu120_subsample10_train100_val10.pth.tar\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid latent dimension. Please choose among 2, 3 and 4. \")\n",
    "    \n",
    "    checkpoint_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    # Load checkpoint into memory\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model = BetaVAE(z_dim=zdim, nc=nc)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(left_surf_data, right_surf_data, zdim, nc, mode, batch_size, device):\n",
    "    \"\"\"\n",
    "    Run inference on left and right surface data using a VAE model.\n",
    "\n",
    "    Parameters:\n",
    "        left_surf_data (np.ndarray or torch.Tensor): Input tensor of shape (batch, C, H, W)\n",
    "        right_surf_data (np.ndarray or torch.Tensor): Same shape as left_surf_data\n",
    "        zdim (int): Dimensionality of latent space\n",
    "        nc (int): Number of input channels\n",
    "        mode (str): \"encode\" for latent output, \"both\" for latent and reconstruction\n",
    "        batch_size (int): Inference batch size\n",
    "        device (torch.device): Target device for model and data\n",
    "\n",
    "    Returns:\n",
    "        If mode == \"encode\":\n",
    "            z_distributions: np.ndarray of shape (N, 2*zdim)\n",
    "        If mode == \"both\":\n",
    "            Tuple of:\n",
    "                z_distributions: np.ndarray of shape (N, 2*zdim)\n",
    "                xL_recon: np.ndarray of shape (N, C, H, W)\n",
    "                xR_recon: np.ndarray of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_loader(left_surf_data, right_surf_data, batch_size):\n",
    "        if isinstance(left_surf_data, np.ndarray):\n",
    "            left_surf_data = torch.tensor(left_surf_data, dtype=torch.float32)\n",
    "        if isinstance(right_surf_data, np.ndarray):\n",
    "            right_surf_data = torch.tensor(right_surf_data, dtype=torch.float32)\n",
    "\n",
    "        # Create a TensorDataset\n",
    "        dataset = TensorDataset(left_surf_data, right_surf_data)\n",
    "        # Return a DataLoader\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        return loader\n",
    "\n",
    "    model = load_model(zdim, nc, device=device)\n",
    "    model.eval()\n",
    "    inference_loader = generate_loader(left_surf_data, right_surf_data, batch_size)\n",
    "\n",
    "    all_z_distributions = []\n",
    "    all_xL_recon = []\n",
    "    all_xR_recon = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xL, xR) in enumerate(inference_loader):\n",
    "            xL = xL.to(device)\n",
    "            xR = xR.to(device)\n",
    "            # print(xL.shape)\n",
    "            z_distribution = model._encode(xL, xR)\n",
    "            all_z_distributions.append(z_distribution.cpu().numpy())\n",
    "\n",
    "            if mode == \"both\":\n",
    "                mu = z_distribution[:, :zdim]\n",
    "                # z = torch.tensor(mu).to(device)\n",
    "                z = mu.clone().detach().to(device)\n",
    "                xL_recon, xR_recon = model._decode(z)\n",
    "                all_xL_recon.append(xL_recon.cpu().numpy())\n",
    "                all_xR_recon.append(xR_recon.cpu().numpy())\n",
    "    \n",
    "    all_z_distributions = np.concatenate(all_z_distributions, axis=0)\n",
    "\n",
    "    if mode == \"encode\":\n",
    "        return all_z_distributions\n",
    "    elif mode == \"both\":\n",
    "        all_xL_recon = np.concatenate(all_xL_recon, axis=0)\n",
    "        all_xR_recon = np.concatenate(all_xR_recon, axis=0)\n",
    "        return all_z_distributions, all_xL_recon, all_xR_recon\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'encode' or 'both'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_etasquared(a, b):\n",
    "    \"\"\"\n",
    "    Calculate eta squared based on Cohen 2008 Neuroimage.\n",
    "    \n",
    "    Parameters:\n",
    "    a : np.ndarray\n",
    "        First input array.\n",
    "    b : np.ndarray\n",
    "        Second input array.\n",
    "    \n",
    "    Returns:\n",
    "    etasquared : np.ndarray\n",
    "        Array of eta squared values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure inputs are at least 2D\n",
    "    if a.ndim == 1:\n",
    "        a = a[:, np.newaxis]\n",
    "    if b.ndim == 1:\n",
    "        b = b[:, np.newaxis]\n",
    "\n",
    "    assert a.shape[0] == b.shape[0], 'input size mismatch'\n",
    "\n",
    "    cols_a = a.shape[1]\n",
    "    cols_b = b.shape[1]\n",
    "    etasquared = np.full((cols_b, cols_a), np.nan)\n",
    "\n",
    "    for ia in range(cols_a):\n",
    "        for ib in range(cols_b):\n",
    "            aa = a[:, ia]\n",
    "            bb = b[:, ib]\n",
    "\n",
    "            m = (aa + bb) / 2\n",
    "            Mbar = np.nanmean(m)\n",
    "            \n",
    "            SSwithin = np.nansum((aa - m) ** 2 + (bb - m) ** 2)\n",
    "            SStotal = np.nansum((aa - Mbar) ** 2 + (bb - Mbar) ** 2)\n",
    "            etasquared[ib, ia] = 1 - SSwithin / SStotal\n",
    "\n",
    "    return etasquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_representations(zs, labels):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_cifti_cohort_filepath = \"./data/cohort_files/cohortfiles_washu120.txt\"\n",
    "example_tmask_cohort_filepath = \"./data/tmask_files/tmasklist_washu120.txt\"\n",
    "\n",
    "cifti_cohort_df = pd.read_csv(example_cifti_cohort_filepath, delim_whitespace=True, header=None)\n",
    "tmask_cohort_df = pd.read_csv(example_tmask_cohort_filepath, delim_whitespace=True, header=None)\n",
    "\n",
    "example_idx = 111\n",
    "subj, cifti1_path = cifti_cohort_df.iloc[example_idx, :2].tolist()\n",
    "tmask_subj, tmask_path = tmask_cohort_df.iloc[example_idx, :].tolist()\n",
    "assert subj == tmask_subj\n",
    "tmask = np.loadtxt(tmask_path, dtype=int).astype(bool)\n",
    "\n",
    "cifti2_path = cifti2_filename(cifti1_path, \"./data/washu120/\")\n",
    "\n",
    "cifti_img = nib.load(cifti2_path)\n",
    "cifti_data = cifti_img.get_fdata()\n",
    "cifti_header = cifti_img.header\n",
    "bm_index_map = cifti_img.header.get_index_map(1)\n",
    "\n",
    "# Initialize storage\n",
    "vertex_indices = []\n",
    "# Loop through the brain models in the index map\n",
    "for bm in bm_index_map.brain_models:\n",
    "    structure = bm.brain_structure\n",
    "    if structure in ['CIFTI_STRUCTURE_CORTEX_LEFT', 'CIFTI_STRUCTURE_CORTEX_RIGHT']:\n",
    "        # Convert Cifti2VertexIndices to numpy array\n",
    "        offset = bm.index_offset\n",
    "        count = bm.index_count\n",
    "        vertex_indices.extend(range(offset, offset + count))\n",
    "\n",
    "vertex_indices = np.array(vertex_indices)\n",
    "cortex_ts = np.transpose(cifti_data[:, vertex_indices])\n",
    "masked_cortex_ts = cortex_ts[:, tmask]\n",
    "corr_matrix = np.corrcoef(masked_cortex_ts)\n",
    "# print(corr_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36864, 29696)\n",
      "(36864, 29716)\n"
     ]
    }
   ],
   "source": [
    "transmat_path = \"./mask\"\n",
    "left_mask_struct = sio.loadmat(os.path.join(transmat_path, \"Left_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "left_mask = left_mask_struct['grid_mapping']\n",
    "right_mask_struct = sio.loadmat(os.path.join(transmat_path, \"Right_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "right_mask = right_mask_struct['grid_mapping']\n",
    "print(left_mask.shape)\n",
    "print(right_mask.shape)\n",
    "# mask = sio.loadmat(os.path.join(transmat_path, \"MSE_Mask.mat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59412, 1, 192, 192)\n",
      "(59412, 1, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "# for test\n",
    "left_surf_data, right_surf_data = forward_reformatting(corr_matrix, transmat_path, img_size=192)\n",
    "print(left_surf_data.shape)\n",
    "print(right_surf_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5941, 1, 192, 192)\n",
      "(5941, 4)\n",
      "(5941, 1, 192, 192)\n",
      "(5941, 1, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "model = load_model(zdim=2, nc=1, device=\"cpu\")\n",
    "left_surf_data, right_surf_data = rand_sample(left_surf_data, right_surf_data, sample_ratio=0.1)\n",
    "print(left_surf_data.shape)\n",
    "zs, left_surf_recon, right_surf_recon = model_inference(left_surf_data, right_surf_data, zdim=2, nc=1, mode=\"both\", batch_size=16, device=\"cpu\")\n",
    "\n",
    "print(zs.shape)\n",
    "print(left_surf_recon.shape)\n",
    "print(right_surf_recon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
