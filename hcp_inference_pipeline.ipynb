{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/wheelock/data1/people/Chenyan/Tu-2025-VAE_FC_embedding/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import scipy.io as sio\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from huggingface_hub import hf_hub_download\n",
    "from fMRIVAE_Model import BetaVAE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# import subprocess\n",
    "# from sklearn.metrics import silhouette_score, davies_bouldin_score, pairwise_distances, silhouette_samples\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "# import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dtseries_and_tmask(subject_id, dtseries_folder, tmask_folder):\n",
    "    \"\"\"\n",
    "    Loads dtseries files and corresponding tmask files for a subject in a fixed order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_id : str\n",
    "        e.g., \"996782\"\n",
    "    dtseries_folder : str\n",
    "        Path to folder containing dtseries files.\n",
    "    tmask_folder : str\n",
    "        Path to folder containing .txt tmask files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    concatenated_data : ndarray (n_vertices, total_timepoints)\n",
    "        Concatenated vertex-level time series (no mask applied).\n",
    "    concatenated_tmask : ndarray (total_timepoints,)\n",
    "        Boolean array indicating which timepoints are valid.\n",
    "    vertex_indices : ndarray\n",
    "        Indices of cortical vertices used.\n",
    "    \"\"\"\n",
    "    run_order = ['REST1_LR', 'REST1_RL', 'REST2_LR', 'REST2_RL']\n",
    "    all_data = []\n",
    "    all_tmask = []\n",
    "    vertex_indices = None\n",
    "\n",
    "    for run in run_order:\n",
    "        # Build filenames\n",
    "        dtseries_path = os.path.join(dtseries_folder, f\"{subject_id}_rfMRI_{run}_surf_subcort_normalwall.dtseries.nii\")\n",
    "        tmask_path = os.path.join(tmask_folder, f\"{subject_id}_rfMRI_{run}_NEW_TMASK.txt\")\n",
    "\n",
    "        # Load dtseries\n",
    "        img = nib.load(dtseries_path)\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        if vertex_indices is None:\n",
    "            bm_index_map = img.header.get_index_map(1)\n",
    "            vertex_indices = []\n",
    "            for bm in bm_index_map.brain_models:\n",
    "                if bm.brain_structure in ['CIFTI_STRUCTURE_CORTEX_LEFT', 'CIFTI_STRUCTURE_CORTEX_RIGHT']:\n",
    "                    vertex_indices.extend(range(bm.index_offset, bm.index_offset + bm.index_count))\n",
    "            vertex_indices = np.array(vertex_indices)\n",
    "\n",
    "        cortex_data = data[:, vertex_indices].T  # (n_vertices, time)\n",
    "        all_data.append(cortex_data)\n",
    "\n",
    "        # Load tmask\n",
    "        with open(tmask_path, 'r') as f:\n",
    "            tmask = np.array([int(line.strip()) for line in f], dtype=bool)\n",
    "        all_tmask.append(tmask)\n",
    "\n",
    "    concatenated_data = np.concatenate(all_data, axis=1)\n",
    "    concatenated_tmask = np.concatenate(all_tmask)\n",
    "\n",
    "    return concatenated_data, concatenated_tmask, vertex_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_masked_ptseries(subject_id, ptseries_folder, parcellation, tmask):\n",
    "    \"\"\"\n",
    "    Loads the ptseries file for a subject and applies reordering and tmask.\n",
    "    Assumes ptseries is already concatenated for REST1 and REST2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject_id : str\n",
    "    ptseries_folder : str\n",
    "    parcellation : dict\n",
    "    tmask : np.ndarray\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ptseries_masked : np.ndarray (n_parcels, time)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import nibabel as nib\n",
    "\n",
    "    pt_file = os.path.join(ptseries_folder, f\"{subject_id}.Rest12.ptseries.nii\")\n",
    "    pt_img = nib.load(pt_file)\n",
    "    pt_data = pt_img.get_fdata()\n",
    "\n",
    "    assert pt_data.shape[0] == tmask.shape[0], \\\n",
    "        f\"Mismatch in ptseries ({pt_data.shape[0]}) and tmask ({tmask.shape[0]}) length\"\n",
    "\n",
    "    parcel_order = parcellation[\"order\"][0, 0].flatten() - 1\n",
    "    pt_ordered = pt_data[:, parcel_order]\n",
    "    return pt_ordered[tmask].T  # shape: (n_parcels, time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_parcel_vertex_correlation(ptseries_masked, cortex_masked):\n",
    "    \"\"\"\n",
    "    Computes Pearson correlation between parcel and vertex time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ptseries_masked : ndarray (n_parcels, n_timepoints)\n",
    "    cortex_masked : ndarray (n_vertices, n_timepoints)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corr_matrix : ndarray (n_parcels, n_vertices)\n",
    "    \"\"\"\n",
    "    # Stack along rows then use np.corrcoef for fast computation\n",
    "    combined = np.vstack([ptseries_masked, cortex_masked])\n",
    "    full_corr = np.corrcoef(combined)\n",
    "    num_parcels = ptseries_masked.shape[0]\n",
    "    corr_matrix = full_corr[:num_parcels, num_parcels:]\n",
    "    return corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_reformatting(corrs, transmat_path, img_size):\n",
    "    \"\"\"\n",
    "    Projects vertex-level data onto 2D cortical grids for left and right hemispheres.\n",
    "\n",
    "    This function:\n",
    "    - Splits vertex-level fMRI data into left and right hemispheres\n",
    "    - Applies hemisphere-specific transformation matrices (e.g., fMRI-to-grid)\n",
    "    - Reshapes the resulting 2D projection into image format for model input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corrs : ndarray of shape (num_samples, num_features)\n",
    "        Vertex-level input data (e.g., correlation values or BOLD signals).\n",
    "        Assumes num_features = 59412 (29696 left + 29716 right cortical vertices).\n",
    "    \n",
    "    transmat_path : str\n",
    "        Directory path containing 'Left_fMRI2Grid_192_by_192_NN.mat' and \n",
    "        'Right_fMRI2Grid_192_by_192_NN.mat', each with a 'grid_mapping' matrix.\n",
    "    \n",
    "    img_size : int\n",
    "        The side length of the 2D grid (e.g., 192 for a 192×192 projection).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    left_surf_data : ndarray of shape (num_samples, 1, img_size, img_size)\n",
    "        2D grid representation of left hemisphere data.\n",
    "    \n",
    "    right_surf_data : ndarray of shape (num_samples, 1, img_size, img_size)\n",
    "        2D grid representation of right hemisphere data.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The transformation matrices are assumed to be of shape (img_size*img_size, num_vertices).\n",
    "    - Data is reshaped using column-major (Fortran-style) ordering to match MATLAB-style layouts.\n",
    "    - Output is formatted in NCHW format for input to deep learning models (e.g., CNNs).\n",
    "    \"\"\"\n",
    "    num_samples, num_vertices = corrs.shape\n",
    "    assert num_vertices == 59412, \"Expected 59412 cortical vertices (29696 left + 29716 right).\"\n",
    "\n",
    "    # Split input features into left and right hemispheres\n",
    "    left_data = corrs[:, :29696]   # shape: (num_samples, 29696)\n",
    "    right_data = corrs[:, 29696:]  # shape: (num_samples, 29716)\n",
    "\n",
    "    # Load transformation matrices\n",
    "    left_transmat = sio.loadmat(os.path.join(transmat_path, \"Left_fMRI2Grid_192_by_192_NN.mat\"))['grid_mapping']\n",
    "    right_transmat = sio.loadmat(os.path.join(transmat_path, \"Right_fMRI2Grid_192_by_192_NN.mat\"))['grid_mapping']\n",
    "\n",
    "    # Project data onto 2D grid\n",
    "    left_proj = left_data @ left_transmat.T    # shape: (num_samples, img_size * img_size)\n",
    "    right_proj = right_data @ right_transmat.T # shape: (num_samples, img_size * img_size)\n",
    "\n",
    "    # Reshape to (num_samples, 1, img_size, img_size) using column-major (Fortran-style) order\n",
    "    # left_surf_data = np.reshape(left_proj, (num_samples, 1, img_size, img_size), order='F')\n",
    "    # right_surf_data = np.reshape(right_proj, (num_samples, 1, img_size, img_size), order='F')\n",
    "    left_surf_data = np.reshape(left_proj, (num_samples, 1, img_size, img_size))\n",
    "    right_surf_data = np.reshape(right_proj, (num_samples, 1, img_size, img_size))\n",
    "\n",
    "    return left_surf_data, right_surf_data\n",
    "\n",
    "def backword_reformatting(left_surf_recon, right_surf_recon, transmat_path):\n",
    "    assert left_surf_recon.shape == right_surf_recon.shape\n",
    "    batch_size = left_surf_recon.shape[0]\n",
    "    left_mask = sio.loadmat(os.path.join(transmat_path, \"Left_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "    right_mask = sio.loadmat(os.path.join(transmat_path, \"Right_fMRI2Grid_192_by_192_NN.mat\"))\n",
    "    left_transmat_backward = left_mask['inverse_transformation']\n",
    "    right_transmat_backward = right_mask['inverse_transformation']\n",
    "\n",
    "    left_corrs = left_transmat_backward @ left_surf_recon.reshape(batch_size, -1).T\n",
    "    right_corrs = right_transmat_backward @ right_surf_recon.reshape(batch_size, -1).T\n",
    "    dtseries_recon = np.vstack((left_corrs, right_corrs))\n",
    "    dtseries_recon[dtseries_recon == 0] = 1\n",
    "    return dtseries_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(zdim, nc, device):\n",
    "    repo_id = \"cindyhfls/fcMRI-VAE\"\n",
    "    if zdim == 2:\n",
    "        filename = \"Checkpoint/checkpoint49_2024-03-28_Zdim_2_Vae-beta_20.0_Lr_0.0001_Batch-size_128_washu120_subsample10_train100_val10.pth.tar\"\n",
    "    elif zdim == 3:\n",
    "        filename = \"Checkpoint/checkpoint49_2024-11-28_Zdim_3_Vae-beta_20.0_Lr_0.0001_Batch-size_128_washu120_subsample10_train100_val10.pth.tar\"\n",
    "    elif zdim == 4:\n",
    "        filename = \"Checkpoint/checkpoint49_2024-06-21_Zdim_4_Vae-beta_20.0_Lr_0.0001_Batch-size_128_washu120_subsample10_train100_val10.pth.tar\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid latent dimension. Please choose among 2, 3 and 4. \")\n",
    "    \n",
    "    checkpoint_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    # Load checkpoint into memory\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model = BetaVAE(z_dim=zdim, nc=nc)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(left_surf_data, right_surf_data, zdim, nc, mode, batch_size, device):\n",
    "    \"\"\"\n",
    "    Run inference on left and right surface data using a VAE model.\n",
    "\n",
    "    Parameters:\n",
    "        left_surf_data (np.ndarray or torch.Tensor): Input tensor of shape (batch, C, H, W)\n",
    "        right_surf_data (np.ndarray or torch.Tensor): Same shape as left_surf_data\n",
    "        zdim (int): Dimensionality of latent space\n",
    "        nc (int): Number of input channels\n",
    "        mode (str): \"encode\" for latent output, \"both\" for latent and reconstruction\n",
    "        batch_size (int): Inference batch size\n",
    "        device (torch.device): Target device for model and data\n",
    "\n",
    "    Returns:\n",
    "        If mode == \"encode\":\n",
    "            z_distributions: np.ndarray of shape (N, 2*zdim)\n",
    "        If mode == \"both\":\n",
    "            Tuple of:\n",
    "                z_distributions: np.ndarray of shape (N, 2*zdim)\n",
    "                xL_recon: np.ndarray of shape (N, C, H, W)\n",
    "                xR_recon: np.ndarray of shape (N, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_loader(left_surf_data, right_surf_data, batch_size):\n",
    "        if isinstance(left_surf_data, np.ndarray):\n",
    "            left_surf_data = torch.tensor(left_surf_data, dtype=torch.float32)\n",
    "        if isinstance(right_surf_data, np.ndarray):\n",
    "            right_surf_data = torch.tensor(right_surf_data, dtype=torch.float32)\n",
    "\n",
    "        # Create a TensorDataset\n",
    "        dataset = TensorDataset(left_surf_data, right_surf_data)\n",
    "        # Return a DataLoader\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        return loader\n",
    "\n",
    "    model = load_model(zdim, nc, device=device)\n",
    "    model.eval()\n",
    "    inference_loader = generate_loader(left_surf_data, right_surf_data, batch_size)\n",
    "\n",
    "    all_z_distributions = []\n",
    "    all_xL_recon = []\n",
    "    all_xR_recon = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xL, xR) in enumerate(inference_loader):\n",
    "            xL = xL.to(device)\n",
    "            xR = xR.to(device)\n",
    "            # print(xL.shape)\n",
    "            z_distribution = model._encode(xL, xR)\n",
    "            all_z_distributions.append(z_distribution.cpu().numpy())\n",
    "\n",
    "            if mode == \"both\":\n",
    "                mu = z_distribution[:, :zdim]\n",
    "                # z = torch.tensor(mu).to(device)\n",
    "                z = mu.clone().detach().to(device)\n",
    "                xL_recon, xR_recon = model._decode(z)\n",
    "                all_xL_recon.append(xL_recon.cpu().numpy())\n",
    "                all_xR_recon.append(xR_recon.cpu().numpy())\n",
    "    \n",
    "    all_z_distributions = np.concatenate(all_z_distributions, axis=0)\n",
    "\n",
    "    if mode == \"encode\":\n",
    "        return all_z_distributions\n",
    "    elif mode == \"both\":\n",
    "        all_xL_recon = np.concatenate(all_xL_recon, axis=0)\n",
    "        all_xR_recon = np.concatenate(all_xR_recon, axis=0)\n",
    "        return all_z_distributions, all_xL_recon, all_xR_recon\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'encode' or 'both'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtseries_folder = \"/data/laumann/data1/tunde/bold_data_final/\"\n",
    "ptseries_folder = \"/data/wheelock/data1/datasets/HCP/HCP_965_all_Gordon333_20221226/parcel_matrices/\"\n",
    "tmask_folder = \"/data/wheelock/data1/datasets/HCP/HCP_all_masks/\"\n",
    "\n",
    "df = pd.read_csv(\"/data/wheelock/data1/datasets/HCP/HCP_965_10min_Gordon333_20221123/retained_FD_Rest1.txt\", delim_whitespace=True)\n",
    "subj_hcpAll = df['subject'].to_list()\n",
    "\n",
    "transmat_path = \"./mask\"\n",
    "parcellation_filename = \"IM_Gordon_13nets_333Parcels.mat\"\n",
    "parcellation = sio.loadmat(os.path.join(transmat_path, parcellation_filename))[\"IM\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_to_vertex_savepath = \"./data/hcp/parcel_to_vertex_corrs/\"\n",
    "latents_savepath = \"./data/hcp/vae_latents/\"\n",
    "zdim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subjects: 100%|██████████| 965/965 [15:04:01<00:00, 56.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipped 0 subjects: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "missing_subjects = []\n",
    "\n",
    "for subj in tqdm(subj_hcpAll, desc=\"Processing subjects\"):\n",
    "    try:\n",
    "        # Load dtseries and tmask\n",
    "        dtseries_data, full_tmask, vertex_indices = load_dtseries_and_tmask(subj, dtseries_folder, tmask_folder)\n",
    "        \n",
    "        # Load ptseries and apply tmask\n",
    "        ptseries_masked = load_masked_ptseries(subj, ptseries_folder, parcellation, full_tmask)\n",
    "\n",
    "        # Compute correlation\n",
    "        corr_matrix = compute_parcel_vertex_correlation(ptseries_masked, dtseries_data[:, full_tmask])\n",
    "\n",
    "        # (Optional) Save or collect corr_matrix here\n",
    "        # np.save(os.path.join(parcel_to_vertex_savepath, f\"{subj}_corrs.npy\"), corr_matrix)\n",
    "        # sio.savemat(os.path.join(parcel_to_vertex_savepath, f\"{subj}_corrs.mat\"), {'corr_matrix': corr_matrix})\n",
    "\n",
    "        l_surf, r_surf = forward_reformatting(corr_matrix, transmat_path=\"./mask\", img_size=192)\n",
    "        zs = model_inference(l_surf, r_surf, zdim=zdim, nc=1, mode=\"encode\", batch_size=16, device=\"cpu\")\n",
    "        mus = zs[:, :zdim]\n",
    "\n",
    "        # np.save(os.path.join(latents_savepath, f\"{subj}_latents.npy\"), mus)\n",
    "        sio.savemat(os.path.join(latents_savepath, f\"{subj}_latents.mat\"), {'mu_distributions': mus})\n",
    "\n",
    "        # print(corr_matrix.shape)\n",
    "        # print(mus.shape)\n",
    "\n",
    "    except (FileNotFoundError, AssertionError, nib.filebasedimages.ImageFileError) as e:\n",
    "        print(f\"Skipping subject {subj} due to error: {e}\")\n",
    "        missing_subjects.append(subj)\n",
    "\n",
    "print(f\"\\nSkipped {len(missing_subjects)} subjects: {missing_subjects}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
